{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Check if Cuda is available\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4uE3sXvIth8",
        "outputId": "32b3faa9-4ae7-410a-db2f-282772417622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 13 17:48:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             50W /  400W |    1083MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.autotune(\n",
        "    configs = [\n",
        "        triton.Config({'BLOCK_SIZE': 128}),\n",
        "        triton.Config({'BLOCK_SIZE': 256}),\n",
        "        triton.Config({'BLOCK_SIZE': 512}),\n",
        "    ],\n",
        "    key = ['seq_len']\n",
        ")\n",
        "@triton.jit\n",
        "def _flash_attention_forward(q_ptr, k_ptr, v_ptr, o_ptr, batch_size, num_heads,\n",
        "                             seq_len, head_dim:tl.constexpr,\n",
        "                             q_stride_b, q_stride_h, q_stride_s, q_stride_d,\n",
        "                             k_stride_b, k_stride_h, k_stride_s, k_stride_d,\n",
        "                             v_stride_b, v_stride_h, v_stride_s, v_stride_d,\n",
        "                             o_stride_b, o_stride_h, o_stride_s, o_stride_d,\n",
        "                             scale: float,\n",
        "                             BLOCK_SIZE: tl.constexpr):\n",
        "  \"\"\"\n",
        "  Compute attention matrix using tiling to avoid materializing the full matrix\n",
        "\n",
        "  Args:\n",
        "     q_ptr, k_ptr, v_ptr, o_ptr: pointers to query, key, value and output matrices\n",
        "     batch_size: number of samples in each batch\n",
        "     num_heads: number of attention heads\n",
        "     seq_len: length of tokens in a sequence\n",
        "     head_dim: number of features in each token\n",
        "     q_stride_b: number of steps for skipping to the next batch\n",
        "     q_stride_h: number of steps for skipping to the next attention head\n",
        "     q_stride_s: number of steps for skipping to the next token in a sequence\n",
        "     q_stride_d: numer of steps for skipping to th next feature\n",
        "     BLOCK_SIZE: a subset of attention matrix, also known as tile\n",
        "     scale: scaling factor for attention scores(typically 1/sqrt(head_dim))\n",
        "  \"\"\"\n",
        "  print(f\"Compiled with BLOCK_SIZE = {BLOCK_SIZE}\")\n",
        "  # Execution grid is defined as (batch_size, num_heads, seq_len)\n",
        "  # Each program instance computes one position in this 3D space\n",
        "  batch_id = tl.program_id(0)\n",
        "  head_id = tl.program_id(1)\n",
        "  seq_id = tl.program_id(2)\n",
        "\n",
        "  # Compute query offset as each program handles one row of data(one query vector)\n",
        "  q_offset = q_stride_b * batch_id + q_stride_h * head_id + q_stride_s * seq_id\n",
        "\n",
        "  # Compute corresponding output offset\n",
        "  o_offset = q_stride_b * batch_id + q_stride_h * head_id + q_stride_s * seq_id\n",
        "\n",
        "  # Load a query vector at position (batch, head, seq)\n",
        "  q = tl.load(q_ptr + q_offset + tl.arange(0, head_dim) * q_stride_d)\n",
        "\n",
        "  # Max attention score so far\n",
        "  m_i = float('-inf')\n",
        "\n",
        "  # Softmax denominator\n",
        "  d_i = 0.0\n",
        "\n",
        "  # Output accumulator\n",
        "  acc = tl.zeros([head_dim], dtype=tl.float32)\n",
        "\n",
        "  # Load key and value vectors in blocks for efficiency\n",
        "  for seq_offset in range(0, seq_len, BLOCK_SIZE):\n",
        "    k_block_offset = k_stride_b * batch_id + k_stride_h * head_id + seq_offset * k_stride_s\n",
        "    v_block_offset = v_stride_b * batch_id + v_stride_h * head_id + seq_offset * v_stride_s\n",
        "\n",
        "    # Handle egde case: the last block might be smaller\n",
        "    k_seq = tl.arange(0, BLOCK_SIZE)\n",
        "    curr_block_size = min(BLOCK_SIZE, seq_len - seq_offset)\n",
        "    mask = k_seq < curr_block_size\n",
        "\n",
        "    # Creates a 2D index space (block_size, head_dim) for loading several key vectors at once\n",
        "    k_indices = k_block_offset + k_seq[:, None] * k_stride_s + tl.arange(0, head_dim)[None, :] * k_stride_d\n",
        "\n",
        "    # Load key vector block\n",
        "    k_block = tl.load(k_ptr + k_indices, mask=mask[:, None], other=0.0)\n",
        "\n",
        "    # Compute attention score Q * K^T\n",
        "    s_ij = tl.sum(q[None, :] * k_block, axis=1)   # Shape: [block_size]\n",
        "    s_ij = tl.where(mask, s_ij, float('-inf'))\n",
        "\n",
        "    # Compute max of current block\n",
        "    m_ij = tl.max(s_ij, axis=0)\n",
        "\n",
        "    if seq_offset == 0:\n",
        "      m_i_new = m_ij\n",
        "    else:\n",
        "      m_i_new = tl.maximum(m_i, m_ij)\n",
        "\n",
        "    # Update softmax denominator\n",
        "    alpha = tl.exp(m_i - m_i_new)\n",
        "\n",
        "    # Precise exp calculation with careful masking\n",
        "    p_ij = tl.exp(s_ij - m_ij)\n",
        "\n",
        "    d_i_new = d_i * alpha + tl.sum(tl.where(mask, p_ij, 0.0), axis=0)\n",
        "\n",
        "    # Load value vector block\n",
        "    v_indices = v_block_offset + k_seq[:, None] * v_stride_s + tl.arange(0, head_dim)[None, :] * v_stride_d\n",
        "    v_block = tl.load(v_ptr + v_indices, mask=mask[:, None], other=0.0)\n",
        "\n",
        "    # Update acc before applying normalization\n",
        "    p_ij_masked = tl.where(mask, p_ij, 0.0)[:, None]  # Add dimension for broadcasting\n",
        "    acc_update = tl.sum(p_ij_masked * v_block, axis=0)\n",
        "    acc = acc * alpha + acc_update\n",
        "\n",
        "    # Update state variable for the next block\n",
        "    m_i = m_i_new\n",
        "    d_i = d_i_new\n",
        "\n",
        "  # Normalize output by the sum of exp\n",
        "  o = acc / (d_i + 1e-5)\n",
        "\n",
        "  # Store final result to output tensor after all blocks are processed\n",
        "  tl.store(o_ptr + o_offset + tl.arange(0, head_dim) * o_stride_d, o)\n",
        "\n"
      ],
      "metadata": {
        "id": "wdQyTrWqiJrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class FlashAttention(torch.nn.Module):\n",
        "  \"\"\"\n",
        "    Flash attention module that implements memory efficient attention computation\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout=0.0) -> None:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      dropout: probability of dropping a fraction of attention scores or outputs\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    if dropout > 0.0:\n",
        "      print(\"Warning: dropout is not implemented in this version!\")\n",
        "\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, q, k, v, scale=None):\n",
        "    \"\"\"\n",
        "      Compute attention score using flash attention algorithm. Assumes sequence length is the same\n",
        "      for query, key and value vectors.\n",
        "\n",
        "      Args:\n",
        "        q(torch.Tensor): query tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
        "        k(torch.Tensor): key tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
        "        v(torch.Tensor): value tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
        "        scale(float): scaling factor for numerical stability, default to 1/sqrt(head_dim)\n",
        "\n",
        "      Returns:\n",
        "        torch.Tensor: output tensor of shape (batch_size, num_heads, seq_len, head_dim)\n",
        "    \"\"\"\n",
        "    # Verify shape consistency in q, k, v\n",
        "    (batch_size, num_heads, seq_len, head_dim) = q.shape\n",
        "    assert k.shape[:3] == (batch_size, num_heads, seq_len), \\\n",
        "      f\"Key tensor shape {k.shape[:3]} doesn't match query tensor.\"\n",
        "    assert v.shape[:3] == (batch_size, num_heads, seq_len), \\\n",
        "      f\"Value tensor shape {v.shape[:3]} doesn't match query tensor.\"\n",
        "\n",
        "    # Last dimension can be different\n",
        "    assert k.shape[3] == v.shape[3] == head_dim, \\\n",
        "      f\"Key, value head dimension {k.shape[3]}, {v.shape[3]} don't match query tensor.\"\n",
        "\n",
        "    if scale == None:\n",
        "      scale = 1.0 / (head_dim ** 0.5)\n",
        "\n",
        "    # Ensure tensors are contiguous in memory for efficient access\n",
        "    q = q.contiguous().to(torch.float32)\n",
        "    k = k.contiguous().to(torch.float32)\n",
        "    v = v.contiguous().to(torch.float32)\n",
        "\n",
        "    q = q * scale\n",
        "\n",
        "    # Initialize an output tensor\n",
        "    o = torch.empty_like(q)\n",
        "\n",
        "    # Get the strides of each tensor\n",
        "    q_stride_b, q_stride_h, q_stride_s, q_stride_d = q.stride()\n",
        "    k_stride_b, k_stride_h, k_stride_s, k_stride_d = k.stride()\n",
        "    v_stride_b, v_stride_h, v_stride_s, v_stride_d = v.stride()\n",
        "    o_stride_b, o_stride_h, o_stride_s, o_stride_d = o.stride()\n",
        "\n",
        "    # Create a 3D execution grid of shape (batch_size, num_heads, seq_len)\n",
        "    grid = (batch_size, num_heads, seq_len)\n",
        "\n",
        "    # Launch the Triton kernel\n",
        "    _flash_attention_forward[grid](q, k, v, o,\n",
        "                             batch_size, num_heads, seq_len, head_dim,\n",
        "                             q_stride_b, q_stride_h, q_stride_s, q_stride_d,\n",
        "                             k_stride_b, k_stride_h, k_stride_s, k_stride_d,\n",
        "                             v_stride_b, v_stride_h, v_stride_s, v_stride_d,\n",
        "                             o_stride_b, o_stride_h, o_stride_s, o_stride_d,\n",
        "                             scale)\n",
        "\n",
        "    return o\n"
      ],
      "metadata": {
        "id": "ZJ4QhTBBRI0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def std_attention(q, k, v):\n",
        "  \"\"\"\n",
        "    Compute attention matrix with standard full kernel materialization.\n",
        "  \"\"\"\n",
        "  scale = 1.0 / (q.shape[3] ** 0.5)\n",
        "\n",
        "  # Step 1: Q * K^T * scale\n",
        "  qk = torch.einsum('bnsd, bntd->bnst', q, k) * scale\n",
        "\n",
        "  # Step 2: apply softmax along the last dim\n",
        "  attn = torch.softmax(qk, dim=-1)\n",
        "\n",
        "  # Step 3: multiply by value vector\n",
        "  out = torch.einsum('bnst, bntd->bnsd', attn, v)\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "UYN_FFDHCdTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def benchmark_attention(q, k, v, flash_attn, iterations=10):\n",
        "    \"\"\"Benchmark standard attention vs flash attention\"\"\"\n",
        "    import time\n",
        "\n",
        "    print(\"\\n----- Performance Benchmark -----\")\n",
        "\n",
        "    # Benchmark standard attention\n",
        "    torch.cuda.synchronize()  # Wait for all CUDA operations to finish\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):  # Run multiple iterations for reliable timing\n",
        "        scale = 1.0 / (q.shape[-1] ** 0.5)\n",
        "        qk = torch.einsum('bnsd,bntd->bnst', q, k) * scale\n",
        "        attn = torch.softmax(qk, dim=-1)\n",
        "        std_output = torch.einsum('bnst,bntd->bnsd', attn, v)\n",
        "    torch.cuda.synchronize()\n",
        "    std_time = (time.time() - start) / iterations\n",
        "\n",
        "    # Benchmark flash attention\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "    for _ in range(iterations):\n",
        "        flash_output = flash_attn(q, k, v)\n",
        "    torch.cuda.synchronize()\n",
        "    flash_time = (time.time() - start) / iterations\n",
        "\n",
        "    print(f\"Standard attention time: {std_time:.4f} s\")\n",
        "    print(f\"Flash attention time: {flash_time:.4f} s\")\n",
        "    print(f\"Speedup: {std_time / flash_time:.2f}x\")"
      ],
      "metadata": {
        "id": "AztAvA2Uan5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage and testing functions\n",
        "def test_flash_attention():\n",
        "  \"\"\"\n",
        "    Test if output from standard attention and flash attention match. CUDA device only.\n",
        "  \"\"\"\n",
        "  # Test dimensions\n",
        "  batch_size = 4\n",
        "  num_heads = 8\n",
        "  seq_len = 512\n",
        "  head_dim = 32\n",
        "\n",
        "  print(f\"Testing with a batch size of {batch_size}, number of attention heads {num_heads}, sequence length {seq_len} and head dimension {head_dim}.\")\n",
        "\n",
        "  # Create random input tensors on GPU\n",
        "  q = torch.randn(batch_size, num_heads, seq_len, head_dim, device=\"cuda\")\n",
        "  k = torch.randn(batch_size, num_heads, seq_len, head_dim, device=\"cuda\")\n",
        "  v = torch.randn(batch_size, num_heads, seq_len, head_dim, device=\"cuda\")\n",
        "\n",
        "  # Compute output with standard attention algorithm\n",
        "  print(\"Computing Standard Attention\")\n",
        "  std_output = std_attention(q, k, v)\n",
        "\n",
        "  # Compute output with flash attention algorithm\n",
        "  print(\"Computing Flash Attention\")\n",
        "  flash_attn = FlashAttention()\n",
        "  flash_output = flash_attn(q, k, v)\n",
        "\n",
        "  # Compare results from both algorithms\n",
        "  max_diff = (flash_output - std_output).abs().max().item()\n",
        "  print(f\"Max difference between flash attention and standard attention: {max_diff}\")\n",
        "\n",
        "  if max_diff < 1e-4:\n",
        "    print(\"Results match within error tolerance.\")\n",
        "  else:\n",
        "    print(\"Results don't match. Check your algorithms!\")\n",
        "\n",
        "  # ----- Benchmark performance -----\n",
        "  benchmark_attention(q, k, v, flash_attn)\n",
        "\n",
        "test_flash_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVbx7hDoRI2Q",
        "outputId": "c422a841-ede6-4591-a563-347aeb062dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with a batch size of 4, number of attention heads 8, sequence length 512 and head dimension 32.\n",
            "Computing Standard Attention\n",
            "Computing Flash Attention\n",
            "Max difference between flash attention and standard attention: 1.4901161193847656e-06\n",
            "Results match within error tolerance.\n",
            "\n",
            "----- Performance Benchmark -----\n",
            "Standard attention time: 0.0004 s\n",
            "Flash attention time: 4.3755 s\n",
            "Speedup: 0.00x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "C4Lw2fh57V4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZjfsdm0UTmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}